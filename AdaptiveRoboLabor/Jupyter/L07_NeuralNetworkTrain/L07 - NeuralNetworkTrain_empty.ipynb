{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/Pics/LOGOS.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labor 07 - Neurális hálózatok tanítása\n",
    "\n",
    "### Kézzel írott számok III.\n",
    "Ebben a feladatban tanítani fogjuk a neurális hálózatunkat, hogy az képes legyen felismerni kézzel írott számjegyeket 0-9-ig."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Adatok beolvasása \n",
    "Importáljuk be a használni kívánt csomagokt és olvassuk be az adatokat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "\n",
    "data = loadmat(\"Lab7data.mat\")                      # adatok beolvasása\n",
    "X = data[\"X\"]\n",
    "y = data [\"y\"]                                      # adatok elrendezése\n",
    "del data\n",
    "m = X.shape[0]\n",
    "print('Shape of X and y in order:')\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "data = loadmat(\"Lab7Weights.mat\")                   # súlyok beolvasása\n",
    "w1 = np.array(data[\"Theta1\"])\n",
    "w2 = np.array(data[\"Theta2\"])                       # súlyok elrendezése\n",
    "print('Shape of w1 and w2 in order:')\n",
    "print(w1.shape)\n",
    "print(w2.shape)\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amit érdemes megfigyelni a kezdeti adatainkon:\n",
    "- A minták egy 5000x400-as mátrixba vannak rendezve, ami azt jenelnti, hogy az 5000 darab 20x20 pixeles képünk van. A képeket kiterítve tároljuk a mátrixban.\n",
    "- Kimenetként egy számot várunk, melyek egy 5000x1-es vektorban vannak eltárolva.\n",
    "- A háló működéséhez szükséges súlymátrixok rendre 25x401 és 10x26. Ez azt jelenti, hogy a 400 kezdeti bemeneti paramáterünk ki van egészítve a BIAS taggal és a rejtett rétegben 25 neuron található.\n",
    "- A rejetett réteghez is csatolva van a BIAS tag és így teremti meg a kapcsolatot a második súlyvektor a 26 rejtett neuton és a 10 lehetséges kimenet között.\n",
    "\n",
    "A háló működését tekintve egy elem 10 osztályba tartozási valószínűségét fogjuk vizsgálni, melyek közül a legnagyobbat választva (MaxPooling) fogjuk meghatározni a végső számot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Adatok vizualizálása"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Printing some random data ...\")                              \n",
    "fig, ax = plt.subplots(10,10, figsize =(8,8))                  # subplots - 10x10 8x8as képnagysággal\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        ax[i,j].imshow(X[np.random.randint(0,m+1),:].reshape(20,20, order = \"F\"), cmap=\"hot\")\n",
    "        ax[i,j].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Elméleti áttekintés\n",
    "\n",
    "A könyebb megértés érdekében tekintsük az alábbi egyszerűsített példát. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/Pics/L07_Network.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A felvázolt neurális háló 3 rétegől áll.\n",
    "- Bemeneti réteg, mely 2 bemeneti neuront tartalmaz plusz a hozzáadott BIAS tagot.\n",
    "- Rejtett réteg, mely 3 neuront tartalmaz plusz a hozzárendelt BIAS tagot.\n",
    "- Kimeneti réteg, mely 3 neuront tartalmaz, melyek közül a maximumot kiválasztva kapjuk meg a tényleges kimenetet.\n",
    "\n",
    "A neuronok szerepét tekintve a BIAS neuronok kezelése kicsit eltérő. Mivel a BIAS nem változó, hanem egy konstans 1-es érték, hogy a hozzá tartozó súly egy független paraméter lehessen. Ennek megfelelően csak a súlyokkal való beszorzásnál van szerepe és minden réteghez igény szerint hozzá rendelhető. \n",
    "\n",
    "Jelöléseket tekintve vezessük be az alábbiakat: <br>\n",
    "$a^{(j)}_{i}$ - j. réteg i. neuronjának aktivációja <br>\n",
    "$s^{(j)}_{i}$ - j. réteg i. neuronjának szumma összege <br>\n",
    "$w^{(j)}_{lk}$ - j. réteg l. neuronjából j+1. rltek k. neuronjába mutató súly (weight) <br>\n",
    "$x^{(m)}_{n}$ - az m. bemeneti minta n. változója. ($x^{(1)}_0 = 1)$ a BIAS tag.<br> \n",
    "$\\hat y$ - a kimenet\n",
    "\n",
    "A forward lépést tehát az alábbiak szerint építhetjük fel: <br>\n",
    "$x^{(1)}$-et kiegészítjük a BIAS taggal és beszorozzuk az első súlymátrixszal.\n",
    "\n",
    "$ \\underset{1\\times 3}{\\mathrm{x^{(1)}}} \\times \\underset{3\\times 3}{\\mathrm{w^{(1)}}} = \\underset{1\\times 3}{\\mathrm{s^{(2)}}} $\n",
    "\n",
    "Elvégezzük a rejtett réteg neuronjaiban az aktivációt. Aktivációs függvénynek használjuk a szigmoid függvényt.\n",
    "\n",
    "$ \\underset{\\color{red}{1\\times 3}}{\\mathrm{a^{(2)}}} = f(\\underset{1\\times 3}{\\mathrm{s^{(2)}}}) = sigmoid(\\underset{1\\times 3}{\\mathrm{s^{(2)}}}) $ \n",
    "\n",
    "Hozzá rendeljük a BIAS tagot a rejtett réteghez az aktiváció után, de még a súly beszorzás előtt!\n",
    "\n",
    "$ \\underset{\\color{red}{1\\times 4}}{\\mathrm{a^{(2)}}} \\times \\underset{4\\times 3}{\\mathrm{w^{(2)}}} = \\underset{1\\times 3}{\\mathrm{s^{(3)}}}  $\n",
    "\n",
    "Az aktivációt követően megkapjuk a kimeneti réteg neuronjoinak értékét. \n",
    "\n",
    "$ \\underset{1\\times 3}{\\mathrm{a^{(3)}}} = f(\\underset{1\\times 3}{\\mathrm{s^{(3)}}}) = sigmoid(\\underset{1\\times 3}{\\mathrm{s^{(3)}}}) $\n",
    "\n",
    "A kimeneti réteg a predikciónkat tartalmazza. \n",
    "\n",
    "$ \\underset{1\\times 3}{\\mathrm{\\hat{y}}} = \\underset{1\\times 3}{\\mathrm{a^{(3)}}} $\n",
    "\n",
    "Klasszifikációs probléma esetén a maximumot, vagyis a legvalószínűbb csoportot kiválasztva kaphatjuk meg a végső predikciót. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Költség függvény\n",
    "\n",
    "Költség függvénynek használjuk az eddig bevált MSE (Mean Square Error) módszert. Nézzük meg egy kimeneti neuron esetében hogyan alakul a hiba számítás. A BIASokat is figyelmen kívül hagyva koncentráljunk csak az ábrán sötét színnel ábrázolt kapcsolatokra. <br>\n",
    "\n",
    "$ C = \\sum\\{\\frac{1}{2}(y-\\color{red}{\\hat y})^2 \\} $\n",
    "\n",
    "$ C = \\sum\\{\\frac{1}{2}(y-{\\color{red}{a^{(3)}}})^2 \\} $\n",
    "\n",
    "$ C = \\sum\\{\\frac{1}{2}(y-f({\\color{red}{s^{(3)}}}))^2\\} $\n",
    "\n",
    "$ C = \\sum\\{\\frac{1}{2}(y-f({\\color{red}{a^{(2)}}}w^{(2)}))^2\\} $\n",
    "\n",
    "$ C = \\sum\\{\\frac{1}{2}(y-f(f({\\color{red}{s^{(2)}}})w^{(2)}))^2\\}$\n",
    "\n",
    "$ C = \\sum\\{\\frac{1}{2}(y-f(f(xw^{(1)})w^{(2)}))^2\\} $\n",
    "\n",
    "A hiba számítás tehát visszavezethető a bemeneti változók és a hálóban szereplő súlyok függvényére. A formula tetszőleges számú rétegre alkalmazható."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hiba visszaterjesztés (Back Propagation)\n",
    "A hiba visszaterjesztés során a kimeneti rétegtől kezdve visszafelé kiszámoljuk, hogy az egyes neuronok mekkora mértékben felelősek a hiba kialakulásáért. Gyakorlatilag a költség függvény kiszámítása során használt lépéseket vissza fele kell elvégeznünk, ami azt jelenti szükségünk lesz az aktivációs függvény deriváltjára. <br>\n",
    "##### Szigmoid függvény és deriváltja\n",
    "$g(z) = \\frac{1}{1+e^{-z}}$ <br>\n",
    "$\n",
    "\\begin{split}\n",
    "g'(z) = \n",
    "& = \\frac{d}{dz}\\frac{1}{1+e^{-z}} \\\\\n",
    "& = \\frac{1}{(1+e^{-z})^2}(e^{(-z)}) \\\\\n",
    "& = \\frac{1}{1+e^{-z}}(1-\\frac{1}{(1+e^{-z})}) \\\\\n",
    "& = g(z)(1-g(z))\n",
    "\\end{split}\n",
    "$\n",
    "<img src=\"files/Pics/L07_SigmoidDeriv.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A hiba visszaterjesztéshes (Back propagation) a költség függvényt a megfelelő súlymátrix elemei szerint parciálisan deriválva kaphatjuk meg a hiba mértékét a rejtett réteg neuronjaiban. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\begin{split}\n",
    "\\frac{\\partial C}{\\partial w^{(2)}} = \\frac{\\partial \\sum \\frac{1}{2}(y-\\hat{y})^2}{\\partial w^{(2)}} = \\sum (\\frac{\\partial \\frac{1}{2}(y-\\hat{y})^2}{\\partial w^{(2)}}) \n",
    "\\end{split}$\n",
    "\n",
    "A könnyebb átláthatóság miatt vezessük le egy elemre a deriválást. <br>\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "\\frac{\\partial \\frac{1}{2}(y-\\hat{y})^2}{\\partial w^{(2)}} \n",
    "& = (y-\\hat{y})(-\\frac{\\hat{y}}{\\partial w^{(2)}}) \\\\\n",
    "& = -(y-\\hat{y}) \\cdot \\frac{\\partial \\hat{y}}{\\partial s^{(3)}}\\cdot \\frac{\\partial \ts^{(3)}}{\\partial w^{(2)}}\\\\\t\n",
    "& = \\color{red}{-(y-\\hat{y}) \\cdot f'(s^{(3)})}\\cdot\n",
    "\\frac{\\partial a^{(2)}w^{(2)}}{\\partial w^{(2)}}\\\\\n",
    "& = {\\color{red} {\\delta^{(3)}}\\cdot a^{(2)}}\n",
    "\\end{split}\n",
    "$<br>\n",
    "\n",
    "Vezesük be $ \\delta^{(j)}_{i} $, mint a j. réteg i. neuronjához rendelt hibatagot. <br>\n",
    "Mátrixos formában kibővítve a dimenziók ellenőrzése után a következő összefüggést kapjuk: <br>\n",
    "$ (a^{(2)})^T \\delta^{(3)} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tanítás\n",
    "A tanító fázis során a korábban megismert módon a súlyokat a Gradiens Descent módszer segítségével módosítjuk. A súlymódosításhoz definiálnunk kell egy tanulási rátát $(\\mu)$ és opcionálisan a büntető tagot is megadhatjuk. <br>\n",
    "\n",
    "$ w^{(1)} = w^{(1)} - \\mu \\frac{\\partial C}{w^{(1)}}+ regularization $\n",
    "\n",
    "$ w^{(2)} = w^{(2)} - \\mu \\frac{\\partial C}{w^{(2)}}+ regularization $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algoritmus lépései\n",
    "Foglaljuk össze az algoritmus megvalósítandó lépéseit\n",
    "\n",
    "1, $\\hspace{2mm} xw^{(1)} = s^{(2)} $\n",
    "\n",
    "2, $\\hspace{14mm} f(s^{(2)}) = a^{(2)}$ \n",
    "\n",
    "3, $\\hspace{32mm} a^{(2)} w^{(2)} = s^{(3)} $\n",
    "\n",
    "4, $\\hspace{48mm} f(s^{(3)}) = \\hat{y} $\n",
    "\n",
    "5, $\\hspace{2mm} C = \\sum\\{\\frac{1}{2}(y-\\hat y)^2 \\} $\n",
    "\n",
    "6, $\\hspace{2mm} -(y-\\hat{y}) \\cdot f'(s^{(3)}) = \\delta^{(3)} $\n",
    "\n",
    "7, $\\hspace{28mm} (a^{(2)})^T \\delta^{(3)} =  \\frac{\\partial C}{\\partial w^{(2)}} $\n",
    "\n",
    "8, $\\hspace{39mm} \\delta ^{(3)} \\cdot (w^{(2)})^T \\cdot f'(s^{(2)})= \\delta ^{(2)} $\n",
    "\n",
    "9, $\\hspace{78mm} x^T \\delta^{(2)} =  \\frac{\\partial C}{\\partial w^{(1)}} $\n",
    "\n",
    "10, $\\hspace{2mm} w^{(1)} = w^{(1)} - \\mu \\frac{\\partial C}{w^{(1)}}+ regularization $\n",
    "\n",
    "$\\hspace{8mm}w^{(2)} = w^{(2)} - \\mu \\frac{\\partial C}{w^{(2)}}+ regularization $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Paraméterek előkészítése"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda = 1                                              # Büntetés mértéke\n",
    "input_layer_size = 400                                  # bemeneti réteg nagysága\n",
    "hidden_layer_size = 25                                  # rejtett réteg nagysága\n",
    "num_labels = 10                                         # labelek száma\n",
    "nn_params = np.append(w1.flatten(), w2.flatten())       # kialapítjuk a w1 és w2-t és egymás után füzzük"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Az architecktúra nyomonkövetése átláthatóbb, ha az egyes rétegek neuronszámait tároljuk a BIAS tagok nélkül. A súlyokat pedig kiterítve egy változóban kezeljük. A mátrixok méretei az egyes rétegek neuron számai alapján visszaállíthatóak. Továbbá szükségünk lesz a tanulási rátára, mint előre beállított paraméterre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Költségfüggvény\n",
    "A költség függvény implementálása előtt szükségünk lesz egy aktivációs függvényre és annak deriváltjára. Aktivációs függvénynek ismét a szigmoid függvényt választjuk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "##########################################    \n",
    "   \n",
    "    \n",
    "##########################################    \n",
    "    return g\n",
    "\n",
    "def sigmoidGradient(z):\n",
    "##########################################    \n",
    "   \n",
    "    \n",
    "##########################################    \n",
    "    return g_deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A költség függvény paraméterként meg fogja kapni a súly paramétereket, a háló rétegméreteit, illetve a mintákat, a mintákhoz tartozó cimkéket és a tanulási rátát. Mivel a súlyparamétereket egy változóban adjuk át a függvénynek, így a függvényen belül kell visszaállítanunk a megfelelő mátrix dimenziókat a súlymátrixoknak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One Hot Encoding\n",
    "\n",
    "A neurális háló kimenetihez igazításhoz szükségünk lesz még egy kis átalakításra. A minták címkéit egy számmal adtuk meg, viszont egy 10 csoportos klasszifikációs problémán dolgozunk. Ennek megfelelően 10 db kimeneti neuronunk lesz, amik azt fogják megadni, hogy mekkora a valószínűsége annak, hogy az adott minta adott osztályba tartozik. A legnagyobb valószínűség alapjén fogjuk tehát meghatározni a prediktált osztályt. Ehhez az átalakításhoz pedig a One Hot Encoding módszert (csak egy 1-es) fogjuk használni. A módszer lényege, hogy minden címkét átalakítunk egy olyan vektorrá, aminek elemeinek száma megegyezik az osztályok számával és csak egy db 1-est tartalmaz a megfelelő osztály oszlopába. \n",
    "\n",
    "Eredeti címkéink a számok 0-9-ig (bal oldalt), a hozzájuk tartozó átalakított címkéink vektorai (jobb oldalt). Jegyezzük meg, hogy az átalakított címkék vektorait összerendezve nagyon hasonlít egy egység mátrixra.\n",
    "\n",
    "<img src=\"files/Pics/L07_OneHot.png\" width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, Lambda):\n",
    "    # nn_params visszaalakítása w1 és w2 mátriokká\n",
    "    w1 = nn_params[:((input_layer_size + 1) * hidden_layer_size)].reshape(hidden_layer_size, input_layer_size + 1)\n",
    "    w2 = nn_params[((input_layer_size + 1) * hidden_layer_size):].reshape(num_labels, hidden_layer_size + 1)\n",
    "\n",
    "    m = X.shape[0]\n",
    "    C = 0\n",
    "    w1_grad = np.zeros((w1.shape))\n",
    "    w2_grad = np.zeros((w2.shape))\n",
    "\n",
    "    # Forward Step\n",
    "    #########################################################################################\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #########################################################################################\n",
    "    \n",
    "    # ONE HOT ENCODING (y to Y)\n",
    "    #########################################################################################\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #########################################################################################\n",
    "\n",
    "    #Penalty\n",
    "    #########################################################################################\n",
    "    \n",
    "    #########################################################################################\n",
    "    \n",
    "    # Cost Function\n",
    "    #########################################################################################\n",
    "    \n",
    "    #########################################################################################\n",
    "\n",
    "    # Back Propagation\n",
    "    #########################################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #########################################################################################\n",
    "    grad = np.append(w1_grad.flatten(), w2_grad.flatten())\n",
    "\n",
    "    return C, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda = 0    \n",
    "C, grad = nnCostFunction(nn_params,input_layer_size,hidden_layer_size,num_labels,X,y,Lambda)\n",
    "print(\"Cost at debugging parameters: %.6f \\nFor Lambda = 0 it should be:  0.287629\" % C)\n",
    "Lambda = 3    \n",
    "C, grad = nnCostFunction(nn_params,input_layer_size,hidden_layer_size,num_labels,X,y,Lambda)\n",
    "print(\"Cost at debugging parameters: %.6f \\nFor Lambda = 3 it should be:  0.576051\" % C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: Kezdeti súlyok\n",
    "\n",
    "A kezdeti súlyokat egyenletes eloszlással inicializáljuk egy meghatározott intervallumon. A súlyok így nem 0 kezdeti értékről indulnak, ami lehetővé teszi a konvergenciát.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randInitializeWeights(L_in, L_out):\n",
    "    epsilon_init = 0.12\n",
    "    W = np.random.rand(L_out,L_in+1)*(2*epsilon_init)-epsilon_init\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w1 = randInitializeWeights(input_layer_size,hidden_layer_size)\n",
    "initial_w2 = randInitializeWeights(hidden_layer_size,num_labels)\n",
    "initial_nn_params = np.append(initial_w1.flatten(),initial_w2.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6: Gradiens módszer\n",
    "\n",
    "A háló tanításához implementáljuk a gradiens módszert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentnn(X, y, initial_nn_params, lr_rate, num_iters, Lambda, input_layer_size, hidden_layer_size,\n",
    "                      num_labels):\n",
    "\n",
    "    w1 = initial_nn_params[:((input_layer_size + 1) * hidden_layer_size)].reshape(hidden_layer_size, input_layer_size + 1)\n",
    "    w2 = initial_nn_params[((input_layer_size + 1) * hidden_layer_size):].reshape(num_labels, hidden_layer_size + 1)\n",
    "\n",
    "    m = len(y)\n",
    "    C_history = []\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        if (i%20==0):\n",
    "            print('Iteration:', i+1)\n",
    "        elif (i==num_iters-1):\n",
    "            print('Done!')\n",
    "        \n",
    "        nn_params = np.append(w1.flatten(), w2.flatten())\n",
    "        C, grad = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, Lambda)\n",
    "        \n",
    "        w1_grad = grad[:((input_layer_size+1) * hidden_layer_size)].reshape(hidden_layer_size,input_layer_size+1)\n",
    "        w2_grad = grad[((input_layer_size +1)* hidden_layer_size ):].reshape(num_labels,hidden_layer_size+1)\n",
    "        \n",
    "        w1 = w1 - (lr_rate * w1_grad)\n",
    "        w2 = w2 - (lr_rate * w2_grad)\n",
    "        C_history.append(C)\n",
    "\n",
    "    nn_paramsFinal = np.append(w1.flatten(), w2.flatten())\n",
    "    return nn_paramsFinal, C_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tanulás paramétereinek beállítása\n",
    "lr_rate = 0.9\n",
    "num_iter = 400\n",
    "Lambda = 0.1\n",
    "\n",
    "nnw, nnC_history = gradientDescentnn(X,y,initial_nn_params,lr_rate,num_iter,Lambda,input_layer_size,hidden_layer_size,num_labels)\n",
    "w1 = nnw[:((input_layer_size+1) * hidden_layer_size)].reshape(hidden_layer_size,input_layer_size+1)\n",
    "w2 = nnw[((input_layer_size +1)* hidden_layer_size ):].reshape(num_labels,hidden_layer_size+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A futtatás paramétereivel (Lambda, tanulási ráta, iterációk száma) lehet kísérletezni.\n",
    "\n",
    "Az adott tanítás sikerességének vizsgálatához implementáljunk egy prediktáló függvényt, ami végrehajtja az előre lépést (Forward Step) és eredményt eredeti alakra alakítja vissza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w1,w2,X):\n",
    "    m = X.shape[0]\n",
    "    #Forward step\n",
    "    ########################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ########################################################\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A kevés mintaszám miatt a tanító adatokon ugyen, de vizsgáljuk meg a háló pontosságát, vagyis vizsgáljuk meg, hogy az összes minta közül hányat talált el megfelelően. Az adatok alapaján adjuk vissza a háló pontosságát százalékban."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred,y):\n",
    "    acc = (np.mean(pred[:,np.newaxis]==y))*100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict(w1,w2,X)\n",
    "acc = accuracy(pred,y)\n",
    "\n",
    "print('Training Set Accuracy after %.0f iteration (currently trained weights): %.2f %%' % (num_iter,acc))\n",
    "\n",
    "w1_r = np.loadtxt(\"w1_final.txt\")\n",
    "w2_r = np.loadtxt(\"w2_final.txt\")\n",
    "\n",
    "pred_800 = predict(w1_r,w2_r,X)\n",
    "acc_800 = accuracy(pred_800,y)\n",
    "print('Training Set Accuracy after 800 iteration (loaded weights): %.2f %%' % acc_800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7: Tanulás lefutása\n",
    "A költségfüggvény interációnkénti kirajzolásával vizsgáljuk meg a tanulás lefutását. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(nnC_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost function')\n",
    "plt.title('Cost function over the iterations (Current params)')\n",
    "plt.show()\n",
    "\n",
    "nnC_history_load=np.loadtxt(\"nnC_history_800.txt\")\n",
    "plt.plot(nnC_history_load)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost function')\n",
    "plt.title('Cost function over the iterations (Loaded params)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magasabb szintű csomagokkal\n",
    "\n",
    "A Keras (vagy Tensorflow) csomagokkal a fentebb részletesen ismertettett eljárások egyszerűbben és tömörebb kódként implementálhatóak. A függvények optimalizáltak. Lássuk a fenti példa hogyan implementálható tömörebben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for array-handling and plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# keras imports for the dataset and building our neural network\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modell létrehozása. Rétegek, neuron számok és aktivációs függvények definiálása."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a linear stack of layers with the sequential model\n",
    "model = Sequential()\n",
    "model.add(Dense(400, input_shape=(400,)))\n",
    "model.add(Activation('sigmoid'))                            \n",
    "\n",
    "model.add(Dense(25))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Költségfüggvény definiálása."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the sequential model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Az eredeti adatokból kiindulva itt is szükség van a One Hot átkódolásra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.zeros((m, num_labels))\n",
    "I = np.eye(num_labels)\n",
    "\n",
    "for i in range(1, m+1):\n",
    "    Y[i-1, :] = I[y[i-1]-1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Háló tanítása, metrikák kinyerése."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model and saving metrics in history\n",
    "history = model.fit(X, Y, epochs=20, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanítás lefutásának ellenőrzése a metrikák segítségével."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the metrics\n",
    "fig = plt.figure()\n",
    "plt.plot(history.history['acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
