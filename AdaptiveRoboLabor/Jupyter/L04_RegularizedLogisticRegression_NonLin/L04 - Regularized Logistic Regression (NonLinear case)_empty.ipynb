{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/Pics/LOGOS.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labor 04: Regulariált Logisztikus Regresszió (NemLineáris eset)\n",
    "### Microchip Anomália:\n",
    "\n",
    "A feladatunk meghatározni, hogy a mikrocsippek közül a mérési eredmények alapján melyik hibás és melyik nem.\n",
    "\n",
    "Jelen gyakorlat során nem lineárisan szeparálható adathalmazzal fogunk dolgozni. A klasszifikációhoz szeretnénk használni a logisztikus regressziót, így a nem lineáris esetet bővítjük több tulajdonság (feature) bevezetésével (polinomiális regresszió).\n",
    "\n",
    "Ebben a feladatban kipróbálásra kerülnek különböző regularizációs paraméterek, hogy jobban megérthessük hogyan működik a regularizálás (büntetés) és miként használható a túltanulás (overfit) megelőzésére. Figyeljük meg a változásokat a döntési határon, ahogy a lamdbát változtatjuk majd. Egy kis lambdával észrevehető lesz, hogy a klaszifikáció során szinte nem hibázik, azonban cserébe nagyon bonyolult görbét kapunk. Ez nem egy jó döntési görbe, vegyük észre, hogy a (-0,25; 1,5) -öt elfogadja, ami egy inkorrekt döntésnek tűnik az adathalmazunk alapján.\n",
    "\n",
    "Egy nagyobb lambda segítségével azt láthatjuk, hogy egy egyszerűbb döntési határ jön létre, ami nem követi annyira az egyes adatokat így ez alultanult (underfitted)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Importáljuk be a megfelelő csomagokat majd alakítsuk ki a megfelelő változókat (X,Y,m,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Lab4data.txt', header = None).to_numpy()        # adatok beolvasása a data változóba NumPy tömbként\n",
    "X = data[:,0:2]                                                     # X rendezése\n",
    "m,n = X.shape                                                       # adatok száma / feature-k száma\n",
    "Y = data[:,2].reshape(m,1)                                          # Y rendezése\n",
    "del data                                                            # felesleges változó törlése\n",
    "\n",
    "print('X:', X.shape)                    # adattömbök méretének / adatok számának / feature-k számának kiírása\n",
    "print('Y:', Y.shape)\n",
    "print('Adatok száma:',m)\n",
    "print('Feature-ök száma:',n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Szedjük ki az elemeket az alapján, hogy átmentek-e a vizsgálaton és rajzoltassuk is ki őket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotData(X,Y):\n",
    "    pos = []                                                        # jó elemek listája \n",
    "    neg = []                                                        # rossz elemek listája\n",
    "\n",
    "    for i in range(0,Y.size):                                       # végig megyünk az Y elemein\n",
    "        if Y[i]==1:                                                 # ha Y értéke 1, akkor X azonos sora megy a pos -ba\n",
    "            pos.append(X[i,:])\n",
    "        elif Y[i]==0:                                               # ha Y értéke 0, akkor X azonos sora megy a neg -be\n",
    "            neg.append(X[i,:])\n",
    "\n",
    "    pos = np.array(pos)                                             # pos legyen NumPy tömb\n",
    "    neg = np.array(neg)                                             # neg legyen NumPy tömb\n",
    "\n",
    "    plt.scatter(pos[:, 0], pos[:, 1], c=\"g\", marker=\"o\", label=\"OK\")        # pos kirajzolása\n",
    "    plt.scatter(neg[:, 0], neg[:, 1], c=\"r\", marker=\"x\", label=\"Not OK\")    # neg kirajzolása\n",
    "    plt.title(\"Training data\")\n",
    "    plt.xlabel(\"Test 1\")\n",
    "    plt.ylabel(\"Test 2\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return pos,neg                                                  # visszaadjuk pos és neg -et a későbbi használatra\n",
    "\n",
    "pos, neg = plotData(X,Y)                                            # függvény meghívása az adatainkra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Láthatjuk, hogy az adataink nem szeparálhatók lineárisan. <br>\n",
    "Mivel adataink mindkét változó szerint $[-1, 1]$ intervallumba esnek elég jó eloszlással, így az adathalmaz további normalizálást nem igényel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Illeszkedések\n",
    "\n",
    "A modell illeszkedését tekintve a mintákra 3 különböző esetet különíthetünk el.\n",
    "\n",
    "- Alul illeszkedésről (Underfit vagy High Bias) akkor beszélünk, amikor a modell túl egyszerű így nagy hibát okoz, mind a tanító adatokon mind a teszt adatokon.\n",
    "\n",
    "- Pont jó az illeszkedés, ha a tanító adatokon és a teszt adatokon is alacsony hibát kapunk. Ez azt jelenti a tanulás során sikerült a lényeges információt megtanulni, ami segítségével az új minták is kellően jól besorolhatóak.\n",
    "\n",
    "- Túltanulásról (Overfit vagy High Variance) beszélünk, ha a tanítás során a modell specifikusan rátanult a tanító mintákra. Ez azt eredményezi, hogy a tanulás során nagyon kis hibaértéket ért el, viszont a tanító adatsorban nem szereplő minták besorolásakor a hiba nagy lesz. \n",
    "\n",
    "<img src=\"files/Pics/L04_Fittings.png\" width=\"800\">\n",
    "\n",
    "Az Underfit és Overfit jelenségek egy tipikus tanulás lefutása során. A cél az lenne, hogy a tanulást azon a ponton állítsuk meg, amikor a validációs hiba a legkisebb. \n",
    "\n",
    "<img src=\"files/Pics/L04_BiasVariance.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gyakorlatban a rendelkezésre álló adatainkat 3 csoportra szoktuk osztani, ha elég nagy mintaszámmal rendelkezünk.\n",
    "- Training set   (~70%): A tanulás során használt adatok, a moddell súlyainak beállítására\n",
    "- Validation set (~15%): A tanulás leállítására a megfelelő epochnál, hiperparaméter optimalizálás\n",
    "- Teszt set      (~15%): Tesztelés független adatokon, metrikák meghatározása\n",
    "\n",
    "Jelen példánk során kis adathalmazzal dolgozunk, így az adatok ilyen fajta szétdarabolásától eltekintünk. Az elméleti kintekintés segíti a megértést.\n",
    "\n",
    "#### Módszerek az Underfit és Overfit esetek kezelésére:\n",
    "\n",
    "Underfit esetén a modellünk túl egyszerű. Megfelelő kompenzáció lehet: <br>\n",
    "- Változók (feature) számának növelése\n",
    "- Komplexebb modell választása\n",
    "\n",
    "Overfit esetén a modellünk túl specifikusan tanul rá a tanító adatokra. Megfelelő kompenzáció lehet: <br>\n",
    "- Kevesebb bementei változó\n",
    "- Egyszerűbb modell\n",
    "- Büntetés (regularizáció) bevezetése\n",
    "\n",
    "Büntetés (Regularizáció) esetén az irány elv a következő: Komplex modellből (pl.: több változó) kiindulva az algoritmusnak számos lehetősége van, ezzel az underfit jelenségét nagy valószínűséggel lekezeltük. A költség függvényt kibővítve pedig büntetjük, ha tól sok változót használ a modell. Ilyen formában megteremtjük az optimális feltételt a feladat megoldásáshoz szükséges legegyszerűbb modell kialakításához. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Bemenetek bővítése\n",
    "\n",
    "Micros chip teszt példánk során a két teszt eredményünk a bemeneti változóink. A kívánt feladat megoldásához több változóra lesz szükségünk. Ennek egy lehetséges módja a bementi változók növelése az eredeti változóink hatványaival. \n",
    "\n",
    "$x_1,\\  x_2 \\Rightarrow\\ 1,\\ x_1,\\ x_2,\\ x_1^2,\\ x_1x_2,\\ x_2^2,\\ x_1^3,\\ x_1^2x_2,\\ x_1x_2^2,\\ x_2^3$\n",
    "\n",
    "A BIAS tagot is beleértve 3-dik hatványig bővítve a bementi változóinkat a kezdeti 2 voltozó helyett 10 változóval számolhatunk.\n",
    "\n",
    "Hozzuk létre a mapFeature() függvényt, ami a fenti leképezést hajtja végre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapFeature(X1,X2,deg):                            # létrehozunk egy függvényt, ami az 'X1' és 'X2' összeg\n",
    "#######################################################\n",
    "                                                        # kombinációját megalkotja (X1, X2, X1^2, X1X2, X2^2,..., X2^3)\n",
    "                                                        # a meghatározott fokszámig (deg - alapesetben 3)\n",
    "\n",
    "    \n",
    "     \n",
    "                                                        # maga az egyes kombinációk létrehozása\n",
    "                                                        # és a függvény kimeneti mátrixába rakása\n",
    "#######################################################            \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg = 3\n",
    "X=mapFeature(X[:,0], X[:,1], deg)                           # hozzuk létre az X bővített változatát\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bemeneti változók kibővítése után rátérhetünk a költség függvény kialakítására."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: Költségfüggvény függvény és Gradiens módszer\n",
    "\n",
    "Az aktivációs függvényünk a szigmoid függvény lesz, ahogy a korábbi laborok során megszokhattuk.\n",
    "\n",
    "A költség függvényt pedig egy büntető taggal bővítjük, az alábbi képlet szerint:\n",
    "\n",
    "$ C(w)=\\frac{1}{2m}\\sum_{i=1}^m(h_w(x^i)-y^i)^2+\\lambda\\sum_{\\color{red}{j=1}}^nw_{\\color{red}{j}}^2 $\n",
    "\n",
    ", ahol <br> \n",
    "$ \\lambda $ a büntetés mértékét beállító paraméter <br>\n",
    "$ i $ a minták indexe 1-től indulva. $i = 1...n$ <br>\n",
    "$ j $ a bemeneti változók indexe 0-tól indulva. $\\color{red}{ j = 0...m}$ <br>\n",
    "\n",
    "Amire figyelni kell, hogy a BIAS-t nem büntetjük, vagyis az $x_{\\color{red}{0}} = 1 $ BIAShoz tartozó $ \\color{red}{w_0}$ súlyt nem szabad a büntetés kiszámításánál figyelembe venni. \n",
    "\n",
    "$ \\lambda $ értékét túl nagyra választva az Underfit esete előjöhet.  \n",
    "\n",
    "Tekintsük át több változós esetben hogyan alakul a költség függvény kibővítve a regularizációs taggal és hogyan fog a gradiens módszerbe illeszkedni.\n",
    "\n",
    "$ C(w)=-\\frac{1}{m}\\sum_{i=1}^{m}y^i\\cdot log(h_w(x^i))+(1-y^i)\\cdot log(1-h_w(x^i))+\\frac{\\lambda}{2m}\\sum_{j=1}^nw_j^2 $\n",
    "\n",
    "A deriválást megkönnyítő megfontolásból a regularizáló tag konstansában $ \\lambda $ helyett $\\frac{\\lambda}{2m}$ szerepel.\n",
    "\n",
    "Gradiens csökkentéses (Gradient Descent) módszer súlyfrissítési alapképlete:\n",
    "\n",
    "$ w_j = w_j - \\mu \\color{blue}{\\frac{\\partial}{\\partial w_j}C(w)}$\n",
    "\n",
    "A költség függvény deriváltjának számolásakor a BIAS eset külön kell kezelnünk. Vizsgáljuk meg $w_0$ és $w_1$ esetén.\n",
    "\n",
    "$ \\color{blue}{\\frac{\\partial}{\\partial w_0}C(w)}=\\frac{1}{m}\\sum_{i=1}^{m}(h_w(x^i)-y^i)\\cdot x_0^i+{\\color{red} 0}$\n",
    "\n",
    "$ \\color{blue}{\\frac{\\partial}{\\partial w_j}C(w)}=\\frac{1}{m}\\sum_{i=1}^{m}(h_w(x^i)-y^i)\\cdot x_j^i+\\frac{\\lambda}{m}w_j $\n",
    "\n",
    "A hasonlóságokat kihasználva gondoljuk át hogyan lehetne egy függvényben kiszámolni mátrixműveletek segítségével a költség függvényt és a gradienst. <br>\n",
    "Implementálja a costFunctionReg() függvényt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunctionReg(w,X,Y,Lambda=1):\n",
    "#######################################################    \n",
    "    \n",
    "    \n",
    "                                                 # aktiváció\n",
    "                                                 # egy alternatív súlymátrixba kinullázzuk a BIAShoz tartozó súlyt\n",
    "                                                 # büntetés\n",
    "\n",
    "                                                 # költségfüggvény\n",
    "\n",
    "                                               # gradiens\n",
    "#######################################################\n",
    "    return C,grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_w = np.zeros((X.shape[1],1))                   # kezdeti súlyok (tiszta 0)\n",
    "C, grad =costFunctionReg(init_w,X,Y)                # költségfüggvény / gradiens számolás\n",
    "print('Expected cost at initial weight (zeros): 0.693')\n",
    "print('Calculated cost at initial weight (zeros): %.4f' % C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A költség függvény és a gradiensek kiszámolásának sikeres implementálása után következhet a súlyok módosítás. <br>\n",
    "\n",
    "Gradient Descent algoritmus súlymódosító alapképlete:\n",
    "\n",
    "$w_j:=w_j-\\mu\\frac{\\partial}{\\partial w_j}C(w)$ \n",
    "\n",
    "Implementálja a gradientDescent() függvényt. Használja fel a costFunctionReg() függvényt és a függvényen belül mentse el minden epoch költségét!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X,Y,w,learning_rate,num_iters,Lambda):                         \n",
    "####################################################    \n",
    "    \n",
    "\n",
    "                                                                            # iterációszámig futtatjuk\n",
    "        \n",
    "                                                                            # súlyok frissítése\n",
    "                                                                            # költségfüggvény történet elmentése\n",
    "\n",
    "    \n",
    "####################################################    \n",
    "    return w, C_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vizsgáljuk meg miként hat a tanulási ráta (learning rate vagy $ \\mu$) és a regularizációs paraméter ($\\lambda $) a költségfüggvény alakulására."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1\n",
    "epoch = 800\n",
    "Lambda = 0.02\n",
    "\n",
    "w, C_history = gradientDescent(X,Y,init_w,learning_rate,epoch,Lambda)      \n",
    "print('\\nRegularized weight:\\n',w)\n",
    "\n",
    "plt.plot(C_history,label = \"C_history\")    \n",
    "plt.title(\"Cost function trough the iterations\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost function value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6: Vizualizáció\n",
    "\n",
    "Rajzoljuk ki a döntési határvonalat az eredeti adatsorunkon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pos[:, 0], pos[:, 1], c=\"g\", marker=\"o\", label=\"OK\")            # eredeti pozitív eredmények rajzolása\n",
    "plt.scatter(neg[:, 0], neg[:, 1], c=\"r\", marker=\"x\", label=\"Not OK\")        # eredeti negatív eredmények rajzolása\n",
    "\n",
    "u_vals = np.linspace(-1,1.,50)                                              # a határ berajzolásához 1 paraméter\n",
    "v_vals = np.linspace(-1,1.,50)                                              # második paraméter\n",
    "z=np.zeros((len(u_vals),len(v_vals)))                                       # eredményváltozó inicializálása\n",
    "\n",
    "for i in range(len(u_vals)):                                                # végigmegyünk az u_vals és v_vals elemein\n",
    "    for j in range(len(v_vals)):\n",
    "        z[i,j] = mapFeature(u_vals[i],v_vals[j],deg) @ w                        \n",
    "\n",
    "plt.contour(u_vals,v_vals,z.transpose(),0)                                  # kirajzoljuk contour plottal a döntési határt\n",
    "plt.title(\"Decision boundary and the training data\")\n",
    "plt.xlabel(\"Exam 1 score\")\n",
    "plt.ylabel(\"Exam 2 score\")\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7: A predikció pontossága"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificationPrediction(w,X):\n",
    "    pred = (sigmoid(X @ w) > 0.5)\n",
    "    return ((np.sum(pred==Y)/m)*100)\n",
    "\n",
    "acc=classificationPrediction(w,X)\n",
    "print('\\nAccuracy of the classification:',acc, '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
