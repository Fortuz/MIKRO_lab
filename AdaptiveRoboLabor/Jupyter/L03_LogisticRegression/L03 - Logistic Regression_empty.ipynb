{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labor 03: Logisztikus regresszió\n",
    "### Egyetemi felvételi:\n",
    "\n",
    "Ebben a feladatban logisztikus regressziót fogunk használni, hogy megjósoljuk egy adott hallgató felvételének tényét az egyetemre.\n",
    "\n",
    "Tegyük fel, hogy egyetemi adminisztrátorok vagyunk és meg szeretnénk határozni,  egy adott jelentkezőnek az esélyét a sikeres felvételire a két felvételi teszt eredménye alapján. Rendelkezésünkre állnak az eddigi eredmények címkézve, hogy az adott hallgató jelentkezése sikeres volt vagy nem.\n",
    "\n",
    "Ezt az adathallmazt tudjuk használni a logisztikus regressziónkra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Importáljuk be a megfelelő csomagokat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Olvassuk be az adatainkat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Lab3data.txt', header = None).to_numpy()       # adatok beolvasása majd NumPy tömbbé alakítása\n",
    "X = data[:,0:2]                                                    # X rendezése\n",
    "m,n = X.shape                                                      # m adatok száma / n feature-k száma\n",
    "Y = data[:,2].reshape(m,1)                                         # Y rendezése\n",
    "del data                                                           # felesleges változó törlése\n",
    "X_original = X\n",
    "\n",
    "print('X:', X.shape)                    # adattömbök méretének / adatok számának / feature-k számának kiírása\n",
    "print('Y:', Y.shape)\n",
    "print('Adatok száma:',m)\n",
    "print('Változók (Feature) száma:',n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Jelenítsük meg az adatainkat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotData(X,Y):\n",
    "    pos=[]                                                  # felvettek (Admit - 1)\n",
    "    neg=[]                                                  # elutasítottak (Denied - 0)\n",
    "\n",
    "    for i in range(0,Y.size):                               # Y alapján végignézzük, hogy felvették e az illetőt\n",
    "        if Y[i] ==0:                                        # ha nem akkor X adott elemei a neg -be mennek\n",
    "            neg.append(X[i,:])\n",
    "        elif Y[i] ==1:                                      # ha igen akkor X adott elemei a pos -ba mennek\n",
    "            pos.append(X[i,:])\n",
    "\n",
    "    neg = np.array(neg)                                     # neg -> NumPy tömb\n",
    "    pos = np.array(pos)                                     # pos -> NumPy tömb\n",
    "\n",
    "    plt.scatter(neg[:,0],neg[:,1],marker='x',c=\"r\", label=\"Not admitted\")   # nem felvettek kirakzolása, x-el, pirosan\n",
    "    plt.scatter(pos[:,0],pos[:,1],marker='o',c=\"g\", label=\"Admitted\")       # felvettek kirajzolása, körrel, zölddel\n",
    "    plt.title(\"Training data\")\n",
    "    plt.xlabel(\"Exam 1 score\")\n",
    "    plt.ylabel(\"Exam 2 score\")\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.show()\n",
    "\n",
    "    return pos, neg                                          # két lista visszaadása\n",
    "\n",
    "pos,neg=plotData(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Láthatjuk, hogy a két kategória elég jól elkülönül."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Adatok előkészítése \n",
    "##### Adatok normalizálása:\n",
    "Ha az adataink egy nagyságrendbe esnek a normalizálás nem feltétlen szükséges, de végre hajtható. (Lásd L02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureNormalization(X):\n",
    "#######################################    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#######################################\n",
    "    return X_norm,mean,std                                 # visszaadjuk a normalizált X-et, átlagot, szórást\n",
    "\n",
    "X,mean,std=featureNormalization(X)                         # el is végezzük a normalizálást"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias hozzáadása az X mátrixhoz:\n",
    "Ahogy már az előzőekben is csináltuk hozzáadunk egy egyesekből álló oszlopot az X mátrixhoz, hogy a tengelymetszet megmaradjon a hipotézisünkben (Lásd. L01 - L02)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "                                            # bias\n",
    "#######################################\n",
    "print(X.shape)                              # visszaellenőrizésnek kiírjuk a mátrix méreteit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: A model:\n",
    "\n",
    "Mivel osztályozási, vagyis klasszifikációs problémával állunk szemben az eddigi lineáris regressziós modellünk nem lesz teljesen megfelelő. Nézzünk egy egyszerű példát. Tumor méret alapján próbáljuk eldönteni, hogy a daganat halálos vagy nem.\n",
    "\n",
    "<img src=\"files/Pics/L03_Tumor.png\" width=\"400\">\n",
    "\n",
    "Ha az eddig bevezetett módszert alkalmazzuk és a\n",
    "\n",
    "$ h_w(x)=XW $ \n",
    "\n",
    "hipotézist használva illesztünk egy egyenest. Az egyenesünk viszont túllövi a korlátos [0,1] halmazt és nem is elég pontos. Elnanyoltan illesztett egyenesünkre ez után egy határértéket definiálva, legyen ez 0.5, el tudjuk dönteni, hogy mi lesz a predikciónk.\n",
    "\n",
    "Ha $h_w(x)\\geq 0.5$, akkor \"y=1\", vagyis halálos. <br>\n",
    "Ha $h_w(x)< 0.5$, akkor \"y=0\", vagyis nem halálos.  <br>\n",
    "\n",
    "A predikciónk viszont eshet 0 alá vagy 1 fölé, ami felesleges. Jobb lenne találni egy korlátos hipotézis függvényt,ami teljesíti az alábbi kritáriumot.\n",
    "\n",
    "$0\\leq h_w(x) \\leq 1$\n",
    "\n",
    "Vezessük be a sigmoid függvényt, mely függvény teljesíti ezt a kritériumot, miszerint korlátos [0,1] tartományon.\n",
    "\n",
    "Szigmoid:\n",
    "\n",
    "$ g(z) = \\frac{1}{1+e^{-z}} $\n",
    "\n",
    "<img src= \"files\\Pics\\L03_sigmoid.png\" width=450>\n",
    "\n",
    "A szigmoid függvény segítségével gyakorlatilag egy valószínűséget rendelünk minden egyes mintához, hogy mekkora annak az esélye, hogy halálos az a méretű tumor. \n",
    "\n",
    "$h_w(x)=P(y=1|X, W)$\n",
    "\n",
    "A hipotézisünket pedig az almábbi módon bővíthetjük: <br>\n",
    "\n",
    "$ h_w(x) = g(XW) $ <br>\n",
    ", ahol $ g(XW) = \\frac{1}{1+e^{-XW}} $\n",
    "\n",
    "$g(XW)\\geq0.5$ <br>\n",
    "akkor, ha $WX\\geq0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Két egyszerű példán vizsgáljuk meg a szigmoid függvénnyel megvalósított osztályozást. <br>\n",
    "\n",
    "#### Lineáris eset:\n",
    "\n",
    "$ h_w(x)=g(w_01+w_1x_1+w_2x_2) $\n",
    "\n",
    "$w=[-3\\ 1\\ 1]$ \n",
    "\n",
    "Predikció: $y=1$ ha $-3+x_1+x_2\\geq0$\n",
    "\n",
    "$x_1+x_2\\geq3$ \n",
    "\n",
    "<img src= \"files\\Pics\\L03_pelda_1.png\" width=200>\n",
    "\n",
    "#### Nem lineáris eset:\n",
    "\n",
    "$ h_w(x)=g(w_01+w_1x_1+w_2x_2+w_3x_1^2+w_4x_2^2) $ \n",
    "\n",
    "$w=[-1\\ 0\\ 0\\ 1\\ 1]$ \n",
    "\n",
    "Predikció: $y=1$ ha $-1+x_1^2+x_2^2\\geq0$\n",
    "\n",
    "$x_1^2+x_2^2\\geq1$\n",
    "\n",
    "<img src= \"files\\Pics\\L03_pelda_2.png\" width=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hozzuk létre a sigmoid függvényt, majd teszteljük is az eredményt -6, 0, 6 értékekre!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "############################################    \n",
    "    \n",
    "############################################    \n",
    "    return g                               # szigmoid függvény értékének visszaadása\n",
    "\n",
    "print('Függvény érték -6 bemenő értékre: %.3f' % sigmoid(-6))           # teszt -6 -ra\n",
    "print('Függvény érték  0 bemenő értékre: %.3f' % sigmoid(0))            # teszt 0 -ra\n",
    "print('Függvény érték  6 bemenő értékre: %.3f' % sigmoid(6))            # teszt 6 -ra \n",
    "\n",
    "if sigmoid(-6) < 0.01 and sigmoid(0) == 0.5 and sigmoid(6) > 0.99:\n",
    "    print(\"\\n A sigmoid() függvény megfelelő. Tovább mehet.\")\n",
    "else:\n",
    "    print(\"\\n Valami nem stimmel. Korrekció szükséges!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 A költségfüggvény:\n",
    "Mivel változtattunk a hipotézis függvényünkkön az eddig használt költség függvényünkön is módosítanunk kell. A feladathoz igazítani. Korábbi példáink során bevezetett MSE megfelelően sima konvergenciát biztosított, a módosított hipotézisünkkel a klasszifikációs problémák megoldásához nem a legjobb választás, mivel sok lokális mininmum ponttal rendelkező nem konvex függvényt fog eredményezni. A kérdés az, tudunk-e találni egy olyan költség függvényt, amely osztályozási feladatok során konvex költség függvényt tud definiálni? Konvex függvényen a gradiens módszerünk sokkal kisebb valószínűséggel fog beragadni lokális minimum pontba.\n",
    "\n",
    "<img src=\"files\\Pics\\L03_Costfunction.png\" width=600>\n",
    "\n",
    "\n",
    "Legyen:\n",
    "\n",
    "$ C(w) = {-log(h_w(x)),\\   ha\\ y=1} $ \n",
    "\n",
    "$ C(w) = {-log(1-h_w(x)),\\ ha\\ y=0} $\n",
    "\n",
    "<img src=\"files\\Pics\\L03_LogCost.png\" width=300>\n",
    "\n",
    "Tehát a költségfüggvényként használjuk az alábbi összefüggést:\n",
    "\n",
    "$ C(w) = -\\frac{1}{m} \\sum{y^i\\cdot\\log(h_w(x^i))-(1-y^i)\\cdot\\log(1-h_w(x^i))} $\n",
    "\n",
    "\n",
    "### 7: A gradiens csökkentés (Gradient Descent Algorithm)\n",
    "Célunk továbbra is a költség függvény minimalizálása. A Gradiens csökkentés továbbra is működő képes, miszerint a súlyokat a költség függvény deriváltjának segítségével csükkentjük.\n",
    "\n",
    "$w_j:=w_j-\\mu\\frac{\\partial}{\\partial w_j}C(w)$ \n",
    "\n",
    "A költségfüggvény parciális deriváltja pedig az eddig megszokott módon számolható:\n",
    "\n",
    "$ \\frac{\\partial}{\\partial w_j}C(w)=\\frac{1}{m}\\sum_{i=1}^{m}(h_w(x^i)-y^i)\\cdot x_j^i $\n",
    "\n",
    "\n",
    "\n",
    "### costFunction definiálása:\n",
    "\n",
    "Írjuk meg a costFunction definícióját, majd teszteljük le két w értékkel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunction(w,X,Y):\n",
    "######################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "######################################################################\n",
    "    return C, grad                                                          # eredmények visszaadása"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(((n+1),1))                                             # kezdeti súlyok (null vektor) létrehozása\n",
    "C1,grad1 = costFunction(initial_w,X,Y)                                      # költségfüggvény tesztelése 1\n",
    "\n",
    "print('''Cost and Gradient  at initial weights (zeros):\n",
    "Expected cost (approx.): 0.693\n",
    "Computed:''',C1)\n",
    "print('''Expected gradient(approx.):\n",
    " [[-0.1]\n",
    " [-0.28]\n",
    " [-0.25]]\n",
    "Computed:\\n''',grad1)\n",
    "\n",
    "test_w = np.array([[-24], [13], [16]])                                    # teszt súlyok létrehozása [-24;0.2;0.2]\n",
    "C2, grad2 = costFunction(test_w,X,Y)                                      # költségfüggvény tesztelése 2\n",
    "print('\\nTest weights:',test_w.transpose())\n",
    "print('''Cost and Gradient  at test weights:\n",
    "Expected cost (approx.): 0.218\n",
    "Computed:''',C2)\n",
    "print('''Expected gradient(approx.):\n",
    " [[-0.44]\n",
    " [-0.14]\n",
    " [-0.06]]\n",
    "Computed:\\n''',grad2)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8: Gradiens csökkentéses módszer definiálása:\n",
    "Páldánk során meghatározott iterációig végezzük az algoritmust a jobb szemléltetés végett."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X,Y,w,learning_rate,num_iters):\n",
    "    C_history = []                                      # C_history változó létrehozása\n",
    "##################################################\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "##################################################\n",
    "    return w, np.array(C_history)                       # visszaadjuk a számolt súlyokat és a kötlségfüggvény történetet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9: Grádiens módszer kirpóbálása több tanulási rátára:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([[0],[0],[0]])     # kezdeti súlyok\n",
    "epoch = 400\n",
    "\n",
    "w_a, C_history_a = gradientDescent(X,Y,w,0.01,epoch)                             \n",
    "plt.plot(range(C_history_a.size), C_history_a, label= \"learning r.:0.01\")\n",
    "                                                       \n",
    "w_b, C_history_b = gradientDescent(X,Y,w,0.1, epoch)                             \n",
    "plt.plot(range(C_history_b.size), C_history_b, label= \"learning r.:0.1\")\n",
    "\n",
    "w, C_history = gradientDescent(X,Y,w,1,epoch)\n",
    "plt.plot(range(C_history.size), C_history, label= \"learning r.:1\")              \n",
    "\n",
    "plt.title(\"Effect of the different constants on the cost function\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost function value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fenti ábrán láthatjuk, hogy a tanulási ráta miként befolyásolta az eredményünket:<br>\n",
    "A tanulási rátát $(\\mu)$ kicsi értéknek választva a konvergencia lassú. <br>\n",
    "A tanulási rátát $(\\mu)$ növelve a konvergencia gyorsul.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('''The cost function at found weights by the gradient descent alg.:\n",
    "Expected (approx): 0.203\n",
    "Computed: %.04f''' % C_history[-1])\n",
    "print('''Weights expected (approx.):\n",
    "[1.658 3.883 3.619]\n",
    "Weights computed: \\n''', w.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9: Kirajzoljuk a döntési határt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pos[:,0],pos[:,1],c=\"g\", marker=\"o\",label=\"Admitted\")     # pos kirajzolása, körökkel, zölddel\n",
    "plt.scatter(neg[:,0],neg[:,1],c=\"r\",marker=\"x\",label=\"Not admitted\")  # neg kirajzolása, x-ekkel, pirossal\n",
    "\n",
    "Exam1_val     = np.array([min(X_original[:, 0])-2, max(X_original[:, 0]+2)]) # döntési határhoz felvesszünk két x értéket\n",
    "Exam1_norm = (Exam1_val - mean[0]) / std[0]\n",
    "Exam2_norm = (-w[0]-w[1]*Exam1_norm)/w[2]                           # kiszámoljuk a hozzájuk tartozó y -t\n",
    "Exam2_val     = (Exam2_norm * std[1]) + mean[1]\n",
    "\n",
    "plt.plot(Exam1_val,Exam2_val,\"k\")                                   # feketével felrajzoljuk a döntési határt\n",
    "plt.title(\"Decision boundary and the training data\")        \n",
    "plt.xlabel(\"Exam 1 score\")\n",
    "plt.ylabel(\"Exam 2 score\")\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10: Predikció\n",
    "A predikció kiszámítása során ügyelnünk kell arra, hogy a mintán ugyan azokat a műveleteket elvégezzük, amiket a tanítás előtt az adat előkészítő fázisban is. Tehát ha normalizáltuk az adatainkat az új adatot is normalizálni kell, illetve a BIAS-t hozzáfűzni. Ezek után az elmentett súlyvektrounkkal kiszámolhatjuk a predikciónkat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X):              # predikciós függvény\n",
    "###########################################    \n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "###########################################    \n",
    "    return p, h                                    \n",
    "\n",
    "NewScore = np.array([45,85])\n",
    "pred, h =predict(NewScore)            # eredmény 45 és 85 pontra\n",
    "print('''Expected result of the prediction with [45 , 85] (approx.):\n",
    "Accepted (1) with 0.767 possibility\n",
    "Predicted: %.0f with %.4f possibility''' % (pred, h[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11: Pontosság\n",
    "Vizsgáljuk meg hogy teljesít a lináris modellunk az eredeti adatok kiértékelése során. Számoljuk ki az algoritmus pontosságát. Ehhez használhatjuk a korábban megírt predict() függvényt vagy az összes mintát egy Batchben is kiértékelhetjük. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateAccuracy():                                    # pontosság függvény\n",
    "#######################################\n",
    "                                                            # minden eredeti X-re kiszámoljuk a predikciót\n",
    "                                                            # ha nagyobb, mint 0.5 akkor pos ellenkező esetben neg\n",
    "#######################################\n",
    "    return accuracy                                         # összehasonlítjuk az Y elemeivel az eredményt és százalékot\n",
    "                                                            # számolunk, ami tükrözi a pontosságot\n",
    "\n",
    "print(float(calculateAccuracy()), '% accuracy (approx. 89.0 % expected)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fejlettebb csomagokkal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression                       \n",
    "\n",
    "data = pd.read_csv('Lab3data.txt', header = None)       # adatok beolvasása\n",
    "XX = data.iloc[:, 0:2].values.reshape(-1, 2)            # adatok szétválogatása\n",
    "YY = data.iloc[:, 2].values.reshape(-1,)                # adatok szétválogatása (LogReg fit-je egy 1d tömböt vár!)\n",
    "\n",
    "logReg = LogisticRegression().fit(XX,YY)\n",
    "test = np.array([[45, 85]])                             # mivel egy mintánk van ezért (1,2) array kell ezért a dupla []\n",
    "pred = logReg.predict(test)                             # sima predikció\n",
    "pred_p = logReg.predict_proba(test)                     # a teszteset valszínűségi predikciója\n",
    "\n",
    "print(\"\"\"Prediction for the approval:\"\"\",int(pred),\"\"\"\n",
    "The value of the probability:\"\"\",pred_p[0,1])\n",
    "\n",
    "acc = logReg.score(XX,YY)                               # pontosság kiszámolása \n",
    "print('Accuracy on the training data:',acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
